---
title: "EP10: Regresión Logística"
date: "2025-06-09"
output: html_document
---

```{r, warning=FALSE, include=FALSE}
# Cargar paquetes
library(dplyr)
library(car)
library(ggpubr)
library(plotly)
library(kableExtra)
library(pROC)
```

## Actividades
> Para este ejercicio usaremos los datos de medidas anatómicas recolectados por Heinz et al. (2003) que ya conocimos en el ejercicio práctico anterior (disponibles en el archivo "EP09 Datos.csv"). Como en este case se requiere de una variable dicotómica, vamos a realizar lo siguiente:

- El equipo crea la variable $IMC$ (índice de masa corporal) como el peso de una persona (en kilogramos) dividida por el cuadrado de su estatura (en metros).
- Si bien esta variable se usa para clasificar a las personas en varias clases de estado nutricional (bajo peso, normal, sobrepeso, obesidad, obesidad mórbida), para efectos de este ejercicio, usaremos dos clases: sobrepeso ($IMC \geq 23,2$) y no sobrepeso ($IMC < 23,2$).
- El equipo crea la variable dicotómica $EN$ (estado nutricional) de acuerdo al valor de $IMC$ de cada persona.

---

> Asegurando reproducibilidad, seleccionar una muestra de 150 mujeres (si su n° de equipo es un número par) o 150 hombres (si su n° de equipo es impar), asegurando que la mitad tenga estado nutricional “sobrepeso” y la otra mitad “no sobrepeso” en cada caso. Dividir esta muestra en dos conjuntos: los datos de 100 personas (50 con EN “sobrepeso”) para utilizar en la construcción de los modelos y 50 personas (25 con EN “sobrepeso”) para poder evaluarlos.

Cómo el número de equipo es 3 (impar), filtramos 150 hombres para el análisis. Además, se crea la variable IMC, y en base a esta se genera adicionalmente la columna EN, que indica si la persona posee sobrepeso.
```{r, warning=FALSE}
# Leer los datos (con separador ; y coma decimal)
datos <- read.csv2("EP09 Datos.csv")

# Convertir columnas numéricas de coma a punto decimal
datos <- datos %>%
  mutate(across(where(is.character), ~ as.numeric(gsub(",", ".", .))))

# Calcular IMC
datos <- datos %>%
  mutate(IMC = Weight / ((Height / 100)^2),
         EN = ifelse(IMC >= 23.2, 1, 0))  # 1 = sobrepeso, 0 = no sobrepeso

# Filtrar solo hombres (Gender == 1)
hombres <- datos %>% filter(Gender == 1)

# Establecer semilla de aleatorización
set.seed(3)
sobrepeso <- hombres %>% filter(EN == 1) %>% sample_n(75)

set.seed(3)
no_sobrepeso <- hombres %>% filter(EN == 0) %>% sample_n(75)


# Combinar la muestra balanceada
muestra <- bind_rows(sobrepeso, no_sobrepeso) %>%
  slice_sample(prop = 1, replace = FALSE)  # mezclar

# Dividir en entrenamiento (100) y prueba (50)
entrenamiento <- bind_rows(
  slice_head(sobrepeso, n = 50),
  slice_head(no_sobrepeso, n = 50)
) %>% slice_sample(prop = 1, replace = FALSE)

prueba <- bind_rows(
  slice_tail(sobrepeso, n = 25),
  slice_tail(no_sobrepeso, n = 25)
) %>% slice_sample(prop = 1, replace = FALSE)


# Posibles predictores obtenidos al azar en la actividad anterior
predictores <- c("Thigh.Girth", "Waist.Girth", "Chest.Girth", "Bitrochanteric.diameter", "Hip.Girth", "Chest.diameter", "Chest.depth", "Age")
```

---

> Seleccionar, de las otras variables, una que el equipo considere que podría ser útil para predecir la clase EN, justificando bien esta selección (idealmente con literatura). 

Se eligió como predictor la variable `Wrist.Minimum.Girth`, se encontró un estudio que encuentra correlación entre el grosor de la muñeca con indicadores de sobrepeso en adultos. *(Wrist Circumference Cutoff Points for Determining Excess Weight Levels and Predicting Cardiometabolic Risk in Adults, Int J Environ Res Public Health. 2024 Apr 26 https://pmc.ncbi.nlm.nih.gov/articles/PMC11120788/)*

---

> Usando el entorno R, construir un modelo de regresión logística con el predictor seleccionado en el paso anterior y utilizando de la muestra obtenida.

```{r, warning=FALSE}
# Falta probar antes peso o altura como predictor (justificar con literarura)
# Si convergen se descarta

modelo <- glm(EN ~ Wrist.Minimum.Girth, 
              data = entrenamiento, 
              family = binomial(link = "logit"))

summary(modelo)

entrenamiento_filtrado<- entrenamiento %>% select(all_of(c("EN", "Wrist.Minimum.Girth", predictores)))
head(entrenamiento_filtrado)

```

---

> Usando estas herramientas para la exploración de modelos del entorno R, buscar entre dos y cinco predictores de entre las variables seleccionadas al azar, recordadas en el punto 2, para agregar al modelo obtenido en el paso 4. Para esto, si su n° de equipo es 3 o 4: utilice eliminación hacia atrás, sin usar la función `step().`

```{r}
modelo_completo <- glm(EN ~ ., data = entrenamiento_filtrado, family = binomial(link = "logit"))


paso_1 <- drop1(modelo_completo, test = "F")
print(paso_1, digits = 3, signif.legend = FALSE)

modelo_1 <- update(modelo_completo, . ~ . - Chest.depth)

# paso 2
paso_2 <- drop1(modelo_1, test = "F")
print(paso_2, digits=3, signif.legend = FALSE)

modelo_2 <- update(modelo_1, . ~ . - Chest.diameter)

# paso 3 
paso_3 <- drop1(modelo_2, test="F")
print(paso_3, digits = 3, signif.legend = FALSE)

modelo_3 <- update(modelo_2, . ~ . - Age)

# paso 4
paso_4 <- drop1(modelo_3, test="F")
print(paso_4, digits = 3, signif.legend = FALSE)

modelo_4 <- update(modelo_3, . ~ . - Waist.Girth)

# paso 5
paso_5 <- drop1(modelo_4, test="F")
print(paso_5, digits = 3, signif.legend = FALSE)

modelo_5 <- update(modelo_4, . ~ . - Thigh.Girth)

# paso 6
paso_6 <- drop1(modelo_5, test="F")
print(paso_6, digits = 3, signif.legend = FALSE)
```

Aunque Wrist.Minimum.Girth es un predictor no significativo, se conservará en el modelo, ya que fue escogido por bibliografía. Por ello, dado que el resto de predictores son significativos y el modelo tiene 4 predictores en total, el modelo 5 será el modelo final.

```{r}
# Modelo final
summary(modelo_5)
```

---

> Evaluar la confiabilidad de los modelos (i.e. que tengan un buen nivel de ajuste y son generalizables) y “arreglarlos” en caso de que tengan algún problema.

### Regresión logística simple

#### 1. Debe existir una relacion lineal entre los predictores y la respuesta transformada.

```{r}
residualPlots(modelo, fitted = FALSE)
crPlots(modelo)
```

Dado el resultado de la prueba de curvatura, con un $p$ de $0.5886$, confirma que se cumple el supuesto de linealidad entre el predictor y la respuesta transformada.

#### 2. Los residuos deben ser independientes entre si.

```{r}
set.seed(3)
durbinWatsonTest(modelo)
```

La prueba de Durbin-Watson resulto no significativa, con un $p$ de $0.9$, por lo que se descarta que no se cumpla la condición de independencia de los residuos.


#### 3. Tamaño de muestra (observaciones suficientes para todas las posibles combinaciones de predictores, en especial para algún nivel de una variable categórica).


Ya que el predictor es numérico, se requiere un mínimo de 15 observaciones, lo cual se cumple ya que se tienen 100 observaciones.

#### 4. No existe separación perfecta.


Al realizar "summary(modelo)", no se reportan advertencias de separación perfecta, por lo que se concluye que no existe este problema.

#### 5. Las estimaciones de los coeficientes del modelo no están dominadas por casos influyentes.

```{r}
influencePlot(modelo, id = list(cex = 0.7))
```

El doble del apalancamiento promedio vendría a ser aproximadamente $2\cdot\frac{1+1}{100}=\frac{1}{50} = 0.02$. Con este valor se puede identificar que los 4 puntos, 23, 27, 75 y 81, no cumplen con la condición de ser menores que el doble del apalancamiento promedio, lo que indica que son puntos influyentes. Sin embargo, al observar los valores correspondientes a la distancia de Cook, estos son sumamente bajos y menores que 1, por lo que no deberían influir en la línea de regresión asociada al RLog simple.

---

### Regresión logística múltiple

#### 1. Debe existir una relacion lineal entre los predictores y la respuesta transformada.

```{r}
residualPlots(modelo_5, fitted = FALSE)
crPlots(modelo_5)
```

Dado los resultados de la prueba de curvatura, reportados por `residualPlots()`, confirman que se cumple el supuesto de linealidad entre los predictores y la respuesta transformada.

#### 2. Los residuos deben ser independientes entre si.

```{r}
set.seed(3)
durbinWatsonTest(modelo_5)
```

La prueba de Durbin-Watson resultó no significativa, con un $p$ de $0.674$, por lo que se descarta que no se cumpla la condición de independencia de los residuos.

#### 3. Multicolinealidad entre los predictores.

```{r}
cat("Inflación de la varianza:\n")
print(vif(modelo_5))
cat("\n")
cat("Valores de tolerancia:\n")
print(1 / vif(modelo_5))
```

Con respecto a la multicolinealidad, se obtienen factores de inflación de varianza bajos, entre 1 y 5 (de hecho, menores a 2.5), de esto se puede concluir que existe una multicolinealidad moderada, pero ya que estos valores son relativamente bajos, no se concluye que sean suficientemente relevantes para causar preocupación, con respecto a la tolerancia, se encuentran valores mayores a 0.4, permitiendo concluir lo mismo que con los factores de inflación.


#### 4. Tamaño de muestra (observaciones suficientes para todas las posibles combinaciones de predictores, en especial para algún nivel de una variable categórica).

Ya que los predictores son numéricos, se requiere un mínimo de 60 observaciones, considerando 15 por cada uno, por lo tanto, se cumple, ya que hay 100 observaciones.

#### 5. No existe separación perfecta.

Al realizar "summary(modelo_5)",no se reportan advertencias de separación perfecta, por lo que se concluye que no existe este problema.

#### 6. Las estimaciones de los coeficientes del modelo no están dominadas por casos influyentes.

```{r}
influencePlot(modelo_5, id = list(cex = 0.7))
```

El doble del apalancamiento promedio vendría a ser aproximadamente $2\cdot\frac{4+1}{100}=\frac{1}{20} = 0.05$. Con este valor se puede identificar que los 4 puntos, 42, 68, 84 y 89, no cumplen con la condición de ser menores que el doble del apalancamiento promedio, lo que indica que son puntos influyentes. Sin embargo, al observar los valores correspondientes a la distancia de Cook, estos son sumamente bajos y menores que 1, por lo que no deberían influir en la línea de regresión asociada al RLog múltiple.


### Bondad de Ajuste de ambos modelos

```{r}
anova_rls <- anova(modelo, test = "LRT")
anova_rlm <- anova(modelo, modelo_5, test = "LRT")

cat("Bondad de ajuste del modelo RLogS:\n")
print(anova_rls)
cat("\n")
cat("Bondad de ajuste del modelo RLogM:\n")
print(anova_rlm)
```
En el RLogS se puede observar una reducción de la devianza de $19.771$ y un p-value significativo de $8.727*10 ^{-6}$ indicando que la adición del predictor "Wrist.Minimum.Girth" mejora el modelo, además el modelo RlogM presenta una disminución de la devianza de $47.839$ y un p-value, también significativo, de  $2.304*10^{-10}$, indicando que la adición de los predictores mejora el modelo significativamente.
De lo anterior se puede concluir que ambos modelos poseen una buena bondad de ajuste.

---

> Usando código estándar, evaluar el poder predictivo de los modelos con los datos de las 50 personas que no se incluyeron en su construcción en términos de sensibilidad y especificidad.

Cómo ambos modelos requieren de la matriz de confusión, exactitud, sensibilidad y especificidad para poder ser evaluados, se estima conveniente hacer una función que encapsule el proceso. Lo mismo aplica para mostrar la matriz de confusión, para mejor visualización se utiliza la librería `kableExtra`.

```{r}
## Función para calcular métricas de clasificación
calcular_metricas <- function(reales, predichos) {
  # Crear matriz de confusión
  vp <- sum(reales == 1 & predichos == 1)
  vn <- sum(reales == 0 & predichos == 0)
  fp <- sum(reales == 0 & predichos == 1)
  fn <- sum(reales == 1 & predichos == 0)
  
  # Calcular métricas
  exactitud <- (vp + vn) / length(reales)
  sensibilidad <- vp / (vp + fn)
  especificidad <- vn / (vn + fp)
  
  return(list(
    matriz = matrix(c(vn, fp, fn, vp), nrow = 2),
    exactitud = exactitud,
    sensibilidad = sensibilidad,
    especificidad = especificidad
  ))
}

mostrar_matriz <- function(matriz, titulo) {
  matriz_df <- as.data.frame(matriz)
  colnames(matriz_df) <- c("Predicción: 0", "Predicción: 1")
  rownames(matriz_df) <- c("Real: 0", "Real: 1")
  
  kbl(matriz_df, caption = titulo) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
    add_header_above(c(" " = 1, "Predicción" = 2))
}
```

### Modelo Simple

#### Cálculo de exactitud, sensibilidad y especificidad

```{r, echo=FALSE, warning = FALSE}



#Comparacion de calidad predictiva de entrenamiento y prueba (RLogS)
cat("\n Muestra de Entrenamiento")
probs_simple_entrenamiento <- predict(modelo, newdata = entrenamiento, type = "response")
preds_simple_entrenamiento<- ifelse(probs_simple_entrenamiento >= 0.5, 1, 0)
metricas_simple_entrenamiento <- calcular_metricas(entrenamiento$EN, preds_simple_entrenamiento)

mostrar_matriz(metricas_simple_entrenamiento$matriz, "Matriz de Confusión - Modelo Simple")
cat("\nExactitud: ", round(metricas_simple_entrenamiento$exactitud, 3))
cat("\nSensibilidad: ", round(metricas_simple_entrenamiento$sensibilidad, 3))
cat("\nEspecificidad: ", round(metricas_simple_entrenamiento$especificidad, 3))

cat("\n\n\n Muestra de Prueba")
probs_simple_prueba <- predict(modelo, newdata = prueba, type = "response")
preds_simple_prueba <- ifelse(probs_simple_prueba >= 0.5, 1, 0)
metricas_simple_prueba <- calcular_metricas(prueba$EN, preds_simple_prueba)

mostrar_matriz(metricas_simple_prueba$matriz, "Matriz de Confusión - Modelo Simple")
cat("\nExactitud: ", round(metricas_simple_prueba$exactitud, 3))
cat("\nSensibilidad: ", round(metricas_simple_prueba$sensibilidad, 3))
cat("\nEspecificidad: ", round(metricas_simple_prueba$especificidad, 3))
```

- Analisis de resutados de prueba
Con una exactitud del $76\%$, sensibilidad del $76\%$ y, coincidentemente, una especificidad del $76\%$. Se puede decir que de 50 casos, acierta en 36; donde de 25 casos reales de sobrepeso, está en lo correcto en 19. Lo mismo ocurre para la especificidad, de 25 casos de no haber indicador de sobrepeso, acierta en 19.

#### ROC y AUC

```{r, warning=FALSE}
roc_simple_entrenamiento <- roc(entrenamiento$EN ~ probs_simple_entrenamiento, direction = "<")
# Curva ROC modelo simple
plot(roc_simple_entrenamiento, main = "Curva ROC - Modelo Simple Entrenamiento", col = "blue", 
     print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, 
     auc.polygon.col = "lightblue", max.auc.polygon = TRUE,
     xlab = "1 - Especificidad", ylab = "Sensibilidad")

roc_simple_prueba <- roc(prueba$EN ~ probs_simple_prueba, direction = "<")
# Curva ROC modelo simple
plot(roc_simple_prueba, main = "Curva ROC - Modelo Simple Prueba", col = "blue", 
     print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, 
     auc.polygon.col = "lightblue", max.auc.polygon = TRUE,
     xlab = "1 - Especificidad", ylab = "Sensibilidad")

```

Al comparar los resultados de la muestra de entrenamiento y de prueba, se puede observar un aumento de la exactitud, especificidad y sensibilidad, además de un aumento en el AUC del gráfico anterior (de $0.74$ a $0.824$), lo que permite concluir que el modelo simple es capaz de generalizarse a la muestra de prueba.

### Modelo Múltiple

#### Cómputo de exactitud, sensibilidad y especificidad

```{r}

#Comparacion de calidad predictiva de entrenamiento y prueba (RLogM)
cat("\n Muestra de Entrenamiento")
probs_multiple_entrenamiento <- predict(modelo_5, newdata = entrenamiento, type = "response")
preds_multiple_entrenamiento <- ifelse(probs_multiple_entrenamiento >= 0.5, 1, 0)
metricas_multiple_entrenamiento <- calcular_metricas(entrenamiento$EN, preds_multiple_entrenamiento)

mostrar_matriz(metricas_multiple_entrenamiento$matriz, "Matriz de Confusión - Modelo Múltiple")
cat("\nExactitud: ", round(metricas_multiple_entrenamiento$exactitud, 3))
cat("\nSensibilidad: ", round(metricas_multiple_entrenamiento$sensibilidad, 3))
cat("\nEspecificidad: ", round(metricas_multiple_entrenamiento$especificidad, 3))

cat("\n\n\n Muestra de Prueba")
probs_multiple_prueba <- predict(modelo_5, newdata = prueba, type = "response")
preds_multiple_prueba <- ifelse(probs_multiple_prueba >= 0.5, 1, 0)
metricas_multiple_prueba <- calcular_metricas(prueba$EN, preds_multiple_prueba)

mostrar_matriz(metricas_multiple_prueba$matriz, "Matriz de Confusión - Modelo Múltiple")
cat("\nExactitud: ", round(metricas_multiple_prueba$exactitud, 3))
cat("\nSensibilidad: ", round(metricas_multiple_prueba$sensibilidad, 3))
cat("\nEspecificidad: ", round(metricas_multiple_prueba$especificidad, 3))
```


- Analisis de resultados de prueba
Con una exactitud del $80\%$, sensibilidad del $76\%$ y una especificidad del $84\%$. Se puede decir que de 50 casos, acierta en 40. La sensibilidad se mantiene respecto al modelo simple, puesto que de los 25 casos reales, acierta en 19. Por otro lado, en la especificidad de los 25 casos donde no se presenta el indicador de sobrepeso, el modelo múltiple acierta en 21.


#### Comparacion RLogS y RlogM

#### ROC y AUC
```{r, warning=FALSE}
roc_multiple_entrenamiento <- roc(entrenamiento$EN ~ probs_multiple_entrenamiento, direction = "<")

plot(roc_multiple_entrenamiento, main = "Curva ROC - Modelo Múltiple Entrenamiento", col = "red", 
     print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, 
     auc.polygon.col = "lightpink", max.auc.polygon = TRUE,
     xlab = "1 - Especificidad", ylab = "Sensibilidad")


roc_multiple_prueba <- roc(prueba$EN ~ probs_multiple_prueba, direction = "<")

plot(roc_multiple_prueba, main = "Curva ROC - Modelo Múltiple Prueba", col = "red", 
     print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, 
     auc.polygon.col = "lightpink", max.auc.polygon = TRUE,
     xlab = "1 - Especificidad", ylab = "Sensibilidad")
```

A diferencia del caso del RLogS, en este se observa que la exactitud se mantiene, la sensibilidad disminuye y la especificidad aumenta, además, se presencia una disminución del AUC de 0.001, de todas maneras los cambios no son de gran magnitud, permitiendo concluir que el modelo puede generalizar de una buena manera.


#### Conclusión
Ambos modelos presentan una buena calidad predictiva, con ambos valores de sensibilidad y especificidad mayores a 70% en los casos de prueba, teniendo mayores valores en el RLogM, pero con una disminución dentro de su sensibilidad entre los casos de entrenamiento y prueba.


